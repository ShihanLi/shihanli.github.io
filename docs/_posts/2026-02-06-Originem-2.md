---
layout: post
title:  "Originem Part 2"
date:   2026-02-06 23:10:43 -0800
categories: essays
---

Last time I left off, I thought the natural progression after implementing a really fancy spaced repetition algorithm that guarantees I can remember anything with minimal review, was to optimize what I was actually learning. 

It was not until the writing of this update that I realized it was not worth my time. 

As a technical person, I'm drawn to the [constructivist idea](https://www.taylorfrancis.com/chapters/edit/10.4324/9780203052600-2/constructivist-approach-teaching-ernst-von-glasersfeld) that learning works best when the content lies just beyond my current knowledge. After adopting the cutting edge [FSRS](https://github.com/open-spaced-repetition/fsrs4anki/wiki/ABC-of-FSRS) algorithm, I now have an accurate model of: 1) the probability I can recall something on any given day, 2) how many days until that probability drops below 90% so I can review in time, and 3) how inherently difficult the content is for me to learn. It's very alluring to leverage what on aggregate is a snapshot of my brain to identify the one new thing to learn next that would unlock maximum understanding in the French corpus.

<div class="bleed images-row">
  <figure>
    <img src="/assets/originem1/homescreen-update.PNG" alt="Homescreen"/>
    <figcaption>Updated homescreen</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/most-stable.PNG" alt="Word list ranked by mastery"/>
    <figcaption>Ranked by mastery</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/most-difficult.PNG" alt="Word list ranked by subjective difficulty"/>
    <figcaption>Ranked by subjective difficulty</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/furthest-due.PNG" alt="Word list ranked by next due"/>
    <figcaption>Ranked by due date (note how long!)</figcaption>
  </figure>
</div>

But whatever marginal effectiveness this N+1 algorithm brings over simply going down a word frequency list is imperceptible. Before learning the first 1000 most common words, just learning the next word is arguably maximally efficient already. After the 1000 words, the benefit of learning another word drops compared to learning word order and word forms (conjugation and tenses), and the rote memorization of flashcards gets boring. Mathematically improving the efficiency of a flashcard app won't help getting a better balance between learning important things and staying engaged in the process.

Another big reason why I made Originem to begin with was to practice more French output. I've always intuitively understood how important output is, and it was nice to confirm when I read [*The Art and Science of Language Teaching*](https://www.cambridge.org/highereducation/books/the-art-and-science-of-language-teaching/B800991707B229AA257285CA87D447B9) that the output with immediate feedback was really all I needed to teach myself practical language skills. 

The book was mostly geared toward classroom teachers, where it's important to design output exercises while accommodating different learning styles and personalities. It seemed obvious that a custom LLM partner could solve most of the problems laid out in the book and those I have with real conversation partners. By understanding what I know and how I learn, it could craft constructive feedback tailored to me. And it would eliminate the scheduling challenges of practicing with a partner, along with the embarrassment and fear of judgement that holds me back.

So I made a prototype.

<div class="bleed images-row">
  <figure>
    <img src="/assets/originem1/speaking1.PNG" alt="Formalities"/>
    <figcaption>Some formalities</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/speaking2.PNG" alt="Conversation"/>
    <figcaption>Conversation flows</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/speaking3.PNG" alt="Misinterpreting book name"/>
    <figcaption>Misinterpreting book name</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/speaking4.PNG" alt="Pronunciation"/>
    <figcaption>Drill pronunciation</figcaption>
  </figure>
</div>

I press a button to say something in French. Once I'm done, an LLM transcribes the audio to text, a second LLM, with access to my conversation context and specialized for language teaching, generates a response, and a third LLM synthesizes that response into natural speech.

It all sounds simple and straightforward, but the experience was less than satisfactory. I learned that the transcription process wasn't error-tolerant, and error tolerance is essential for being learning friendly. The LLM was bad at recognizing beginner, accented speech correctly or detecting when I switched languages mid sentence to compensate for missing vocabulary, resulting in speech to text output that was mostly gibberish. This was frustrating from a learning perspective and, surprisingly, an emotional one as well. It recreated the same discomfort and self-consciousness I feel when practicing with a real person.

A less challenging but still significant problem is that the AI feels robotic. I have to restart every conversation from scratch and there's no real back and forth since the AI can't really respond well to "what are you up to?". Overall, I didn't feel motivated to practice speaking as much as I should. 

The easy fix was to track conversation history and remember facts that I've mentioned like ChatGPT does and give the AI a personality. I'll likely work on this next, along with collapsing the three LLM steps into a smarter multimodal LLM like [Sesame](https://www.sesame.com/) does to hopefully improve error tolerance. 

Writing, on the other hand, had none of the problems the speaking module had. I had wanted to write about whatever comes to mind througout the day. Right after I send an email, I often think if I could have written it in French. Or I'll try to describe what I'm planning for the evening or the weekend. It's so much lower effort, almost equivalent to a Google search, that I found myself using it on the walk from the office to lunch.

<div class="bleed images-row">
  <figure>
    <img src="/assets/originem1/writing1.PNG" alt="Learn from feedback"/>
    <figcaption>Learn from feedback</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/writing2.PNG" alt="Cloze flashcard added"/>
    <figcaption>Cloze flashcard added</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/writing3.PNG" alt="Flashcard front"/>
    <figcaption>Flashcard front</figcaption>
  </figure>
  <figure>
    <img src="/assets/originem1/writing4.PNG" alt="Flashcard back"/>
    <figcaption>Flashcard back</figcaption>
  </figure>
</div>

AI is inherently better at understanding errors in writing since there's no degree of separation, like the transcription layer, between the AI and the original material that could introduce mistakes. I make all sorts of spelling and grammatical errors in most of my sentences, and I mix in English or Chinese sometimes. An example I've actually written is "Je resentir? tres biens cet matin". But the model has no problem understanding what I'm trying to say despite the errors. It gives constructive feedback on the most authentic ways of saying what I wanted, and I find that extremely satisfying.

Some other reasons I love the writing exercises so much: First is the endowment effect, that we value things more when we choose to learn them or help create them. Second, and somewhat related, is that because I wanted to express a particular thought, it's the most relevant and customized to my learning, and by definition the most useful thing for me to learn. And finally, the immediate feedback that I get, plus the ability to save parts of that feedback to my spaced repetition program, captures the pure satisfaction of learning itself. 

And this satisfaction of self-driven learning is what I'm focusing on next. I want to make the app more dynamic and customized to me through creating more ways to save things to spaced repetition, and more types of flashcards that match the shape of the knowledge being learned. For example, cloze deletions for grammar patterns or listening flashcards for understanding liasons.

<p class="thanks">
  Thank you to <a href="https://harukanoishiki.wordpress.com/">Haruka</a> for all the Originem feedback and for having a longer streak than I do.
</p> 